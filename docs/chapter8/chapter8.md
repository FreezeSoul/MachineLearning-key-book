# 第8章：遗憾界

*Edit: Hao ZHAN，赵志民*

---

本章的内容围绕学习理论中的遗憾（regret）概念展开（有的教材里也翻译为“悔” ）。
通常，我们使用超额风险（excess risk）来评估批量学习的分类器性能，而用遗憾来评估在线学习的分类器性能。
二者的不同在于，前者衡量的是整个学习过程结束后所得到的分类器性能，可以理解为学习算法**最终输出的模型**与假设空间内**最优模型**的风险之差；
而后者衡量的是算法运行过程中，所产生的**模型**与假设空间内**最优模型**的损失之差的**和**。



## 1. 【概念补充】超额风险与遗憾的区别

8.1介绍了遗憾这一评估指标的基本概念，我们在此基础上梳理一下其与超额风险这一评估指标的区别。

超额风险这一评估指标被定义为，
$$
ER = \mathbb{E}_{(x,y)\sim D[l(w_{T+1},(x,y))]} - min_{w \in W} \mathbb{E}_{(x,y)\sim D[l(w,(x,y))]}
$$
其中，$ER$ 指的是excess risk，等式右边的前半部分 $\mathbb{E}_{(x,y)\sim D[l(w_{T+1},(x,y))]}$ 指的是模型 $w_{T+1}$ 的风险，等式右边的后半部分 $min_{w \in W} \mathbb{E}_{(x,y)\sim D[l(w,(x,y))]}$ 指的是假设空间内的最优模型的风险。值得注意的是，这里的评估是在整个数据集上进行的，也正是因为如此，我们必须要引入期望的操作。

而遗憾这一评估指标，被定义为，
$$
regret = \sum^{T}_{t=1}f_t(w_t)-min_{w\in W}\sum^{T}_{t=1}f_t(w)
$$
其中，$f_t(w_t)$ 指的是，
$$
\sum^{T}_{t=1}l(w_t,(x_t,y_t)) - min_{w \in W}\sum^{T}_{t=1}l(w,(x_t,y_t))
$$
由于$w_t$的计算过程与样本$(x_t,y_t)$ 无关，而是与$(x_1,y_1)...(x_{t-1},y_{t-1})$ 有关，因此可以直接使用 $l(w,(x_t,y_t))$ 来衡量性能。

由此，我们可以总结出二者之间的两个主要区别。一是超额风险引入了**期望**而遗憾没有；二是超额风险计算是一次性在所有数据上进行的计算，而遗憾是对多次损失的一个**求和**。同时，由于在线学习不依赖于任何分布假设，因此其适用于一系列样本并非 i.i.d ，或者才样子固定分布的情形。



## 2. 【定理补充】随机多臂赌博机遗憾界

**P172**中定理8.3给出了随机多臂赌博机的遗憾界，我们在此基础上对部分证明过程进行补充。

首先，（8.42）给出当$\overline{\mu}_*(p)+\sqrt{\frac{2\ln t}{p}}\le\overline{\mu}_i(q)+\sqrt{\frac{2\ln t}{q}}$成立时，必然有一个成立的三种可能情况。
但是这三种情况并不是互斥的，因此显得很不直观，这里把第二种情况做了细微调整，即：
$$
\overline{\mu}_*(p)+\sqrt{\frac{2\ln t}{p}}\le\mu_*,\mu_*\le\overline{\mu}_i(p)+\sqrt{\frac{2\ln t}{q}},\overline{\mu}_i(p)+\sqrt{\frac{2\ln t}{q}}\le\overline{\mu}_i(q)
$$
此时，构造（8.44）和（8.45）的逻辑就显得更为顺畅。
我们令$\ell=\lceil(2\ln T)/\Delta_i^2\rceil$，则（8.45）转化为：
$$
P(\mu_*\le\mu_i+\sqrt{\frac{2\ln t}{q}})=0,q\ge\ell
$$
代入（8.44），可得：
$$
\begin{aligned}
\mathbb{E}[n_i^T]&\le\lceil\frac{2\ln T}{\Delta_i^2}\rceil+2\sum_{t=1}^{T-1}\sum_{p=1}^{t-1}\sum_{q=\ell}^{t-1}t^{-4} \\
&\le\frac{2\ln T}{\Delta_i^2}+1+2\sum_{t=1}^{T-1}\sum_{p=1}^{t}\sum_{q=1}^{t}t^{-4} \\
&\le\frac{2\ln T}{\Delta_i^2}+1+2\lim_{T\rightarrow+\infty}\sum_{t=1}^{T-1}t^{-2} 
\end{aligned}
$$
根据$p$-级数判别法，当$p=2\gt1$时，级数收敛，因此$\lim_{T\rightarrow+\infty}\sum_{t=1}^{T-1}t^{-2}$是有界的。
至于该级数的具体值，对定理的结论并没有影响，因此我们可以直接将其视为一个常数，然后带入后续的推导过程中。
不过这里出于证明完整性的考虑，我们对此进行简要说明。

$\lim_{T\rightarrow+\infty}\sum_{t=1}^{T}t^{-2}$的取值在数学界被称为巴塞尔问题，证明过程涉及诸多前置定理，故我们给出另一种缩放的方法，即：
$$
\begin{aligned}
\sum_{t=1}^{T-1}t^{-2}&\le1+\int_{1}^{T-1}\frac{1}{x^2}dx \\
&=1+(-\frac{1}{x})|_1^{T-1} \\
&=2-\frac{1}{T}
\end{aligned}
$$
对不等式两边同时取极限，可得：
$$
\lim_{T\rightarrow+\infty}\sum_{t=1}^{T-1}t^{-2}\le2
$$
代入（8.46），一样可以得到类似（8.47）的结论。

这里依旧沿用$\lim_{T\rightarrow+\infty}\sum_{t=1}^{T}t^{-2}=\frac{\pi^2}{6}$的结论，得到遗憾界（8.47），即：
$$
\mathbb{E}[regret]\le\sum_{i=1}^{K}\frac{2\ln T}{\Delta_i^2}+O(1)
$$

此时（8.46）变成：
$$
\mathbb{E}[n_i^T]\le\sum_{i\neq*}^K\frac{2\ln T}{\Delta_i}+(1+\frac{\pi^2}{3}){\Delta_i}=O(K\log T)
$$
观察（8.47）可知，求和公式中的每一项符合对钩函数的构造，即：
$$
f(x)=Ax+\frac{B}{x},x\gt0,A\gt0,B\gt0
$$
这里$x=\Delta_i,A=1+\frac{\pi^2}{3},B=2\ln T$，因此无论$\Delta_i$过大或过小时，都会导致遗憾界的上界变大。
