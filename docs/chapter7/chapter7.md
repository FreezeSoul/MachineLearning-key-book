# 第7章：收敛率

*Edit: 李一飞，赵志民*

---

本章的内容围绕学习理论中的算法收敛率（convergence rate）展开。具体来说，将会考察确定优化下的收敛率问题，以及随机优化下的收敛率问题，并在最后分析支持向量机的实例。

## 1.【概念补充】算法收敛率

在算法分析中，收敛率是指迭代算法逼近解或收敛到最优或期望结果的速度，它衡量算法在减小当前解与最优解之间差异方面的快慢。

设 $\{x_k\}$ 是算法生成的迭代序列，我们可以根据以下公式来衡量算法的收敛率：
$$
lim_{t\rightarrow+\infty}\frac{||x_{t+1} - x^*||}{||x_t - x^*||^p} = C 
$$
其中，$C$为收敛因子，$p$为收敛阶数，$x^*$ 表示最优解，$||.||$ 表示适当的范数。

进而，我们可以将收敛率分为以下几种情况：
1. 超线性收敛：$p\ge1$，$C=0$，表明每次迭代都会使得误差减小，且减小的速度越来越快。
特别地，当$p>1$时，称为$p$阶收敛。例如，p=2时，称为平方收敛；p=3时，称为立方收敛。
2. 线性收敛：$p=1$，$C>0$，表明每次迭代都会使得误差减小（误差呈几何级数下降），但减小的速度是一定的。
3. 次线性收敛：$p=1$，$C=1$，表明每次迭代都会使得误差减小，但减小的速度越来越慢。
