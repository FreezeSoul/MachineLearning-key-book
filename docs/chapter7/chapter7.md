# 第7章：收敛率

*Edit: 李一飞，赵志民*

---

本章的内容围绕学习理论中的算法收敛率（convergence rate）展开。具体来说，将会考察确定优化下的收敛率问题，以及随机优化下的收敛率问题，并在最后分析支持向量机的实例。

## 1.【概念补充】算法收敛率

在算法分析中，收敛率是指迭代算法逼近解或收敛到最优或期望结果的速度，它衡量算法在减小当前解与最优解之间差异方面的快慢。

设 $\{x_k\}$ 是算法生成的迭代序列，我们可以根据以下公式来衡量算法的收敛率：
$$
lim_{t\rightarrow+\infty}\frac{||x_{t+1} - x^*||}{||x_t - x^*||^p} = C 
$$
其中，$C$为收敛因子，$p$为收敛阶数，$x^*$ 表示最优解，$||.||$ 表示适当的范数。

进而，我们可以将收敛率分为以下几种情况：
1. 超线性收敛：$p\ge1$，$C=0$，表明每次迭代都会使得误差减小，且减小的速度越来越快。
特别地，当$p>1$时，称为$p$阶收敛。例如，$p=2$时，称为平方收敛；$p=3$时，称为立方收敛。
2. 线性收敛：$p=1$，$C>0$，表明每次迭代都会使得误差减小（误差呈几何级数下降），但减小的速度是一定的。
3. 次线性收敛：$p=1$，$C=1$，表明每次迭代都会使得误差减小，但减小的速度越来越慢。



## 2.【定理补充】凸函数确定优化

我们发现，书中给出的梯度下降算法将$T$轮迭代的均值作为输出而不是以$\omega_T$作为最终结果。
这是因为在凸函数的梯度下降时，我们设定的步长$\eta$是启发式的，因此每次迭代产生的$\omega'$无法保证是局部最优解。
考虑到定理7.1的结论，$T$轮迭代的$\omega$均值具有次线性收敛率，而我们却无法证明最后一次迭代值$\omega_T$也具有与之相较的收敛率。
总之，返回$\omega$的均值可能会提高计算的代价，但却可以确保稳定的收敛率。该思想在7.3.1和7.3.2中梯度下降算法中亦有体现。

作为对比，在7.2.2中强凸函数的梯度下降算法中，我们只输出了最后一次迭代值$\omega_T$。
这是因为在强凸函数的条件下，每次迭代的梯度更新均有闭式解：$\omega_{t+1}=\omega_t-\frac{1}{\gamma}\nabla f(\omega_t)$。
每次迭代无需任何启发式算法就可以得到该临域的全局最优解，这也是此算法拥有更快收敛率（线性收敛率）的原因。因而，无需返回历史$\omega$的均值。

另外，在（7.12）的推导中，利用了第一章补充内容 AM-GM 不等式$n=2$的结论，即对于任意非负实数$x,y$，有
$$
\sqrt{xy}\le\frac{x+y}{2}
$$
当且仅当$x=y$时取等号。

因此只有满足$\frac{\Gamma^2}{2\eta T}=\frac{\eta l^2}{2}$时，$\frac{\Gamma^2}{2\eta T}+\frac{\eta l^2}{2}$才能取得最小值$\frac{l\Gamma}{\sqrt T}$，此时我们只需设置步长$\eta=\frac{\Gamma}{l\sqrt T}$即可。
类似的推导可以在（7.35）和（7.39）中找到。