# 序言

*编辑：詹好，赵志民，王茂霖*

---

## 关于《机器学习理论导引》

近年来，机器学习领域发展迅猛，相关的课程与教材层出不穷。国内的经典教材如周志华的 [《机器学习》](https://book.douban.com/subject/26708119) 和李航的 [《统计学习方法》](https://book.douban.com/subject/33437381)，为许多学子提供了机器学习的入门指引。而在国外，Tom Mitchell 的 *Machine Learning*、Richard O. Duda 等人的 *Pattern Classification*、Ethem Alpaydın 的 *Introduction to Machine Learning* 等书籍则提供了更为系统的学习路径。对于希望深入学习的读者，Christopher M. Bishop 的 *Pattern Recognition and Machine Learning*、Kevin P. Murphy 的 *Machine Learning - A Probabilistic Perspective*、Trevor Hastie 等人的 *The Elements of Statistical Learning* 等著作也能提供详尽的理论指导。这些书籍无论在国内外，都成为了学习机器学习的重要资源。

然而，从**机器学习理论**的角度来看，现有的学习材料仍存在不足之处。相比于聚焦机器学习算法的著作，专注于机器学习理论的书籍未得到足够的重视。尽管上述一些经典著作中涉及到理论探讨，但篇幅有限，往往仅以独立章节或片段呈现，难以满足深入研究的需求。

以往的机器学习理论经典教材大多为英文撰写。上世纪末围绕统计学习理论展开的讨论，催生了诸如 Vladimir Vapnik 的 *The Nature of Statistical Learning Theory* 和 *Statistical Learning Theory*，以及 Luc Devroye 等人的 *A Probabilistic Theory of Pattern Recognition* 等经典文献。近年来，Shai Shalev-Shwartz 和 Shalev Ben-David 的 *Understanding Machine Learning*，以及 Mehryar Mohri 等人的 *Foundations of Machine Learning* 进一步推进了这一领域的发展。虽然部分经典著作已有高质量的中文译本，但由中文作者撰写的机器学习理论入门书籍仍显不足。

如今，周志华、王魏、高尉、张利军等老师合著的 [《机器学习理论导引》](https://book.douban.com/subject/35074844)（以下简称《导引》）填补了这一空白。该书以通俗易懂的语言，为有志于学习和研究机器学习理论的读者提供了良好的入门指引。全书涵盖了 **可学性、假设空间复杂度、泛化界、稳定性、一致性、收敛率、遗憾界** 七个重要的概念和理论工具。

尽管学习机器学习理论可能不像学习算法那样能够立即应用，但只要持之以恒，深入探究，必将能够领悟到机器学习中的重要思想，并体会其中的深邃奥妙。

-- *詹好*

## 关于《机器学习理论导引》的讲解与笔记

《导引》的讲解笔记在团队内部被亲切地称为《钥匙书》。“钥匙”寓意着帮助读者开启知识之门，解答学习中的疑惑。

《导引》作为一本理论性较强的著作，涵盖了大量数学定理和证明。尽管作者团队已尽力降低学习难度，但由于机器学习理论本身的复杂性，读者仍需具备较高的数学基础。这可能导致部分读者在学习过程中感到困惑，影响学习效果。此外，由于篇幅限制，书中对某些概念和理论的实例说明不足，也增加了理解的难度。

基于以上原因，我们决定编辑这本《钥匙书》作为参考笔记，对《导引》进行深入的注解和补充。其目的是帮助读者更快理解并掌握书中内容，同时记录我们在学习过程中的思考和心得。

《钥匙书》主要包含以下四个部分：

1. **概念解释**：介绍书中涉及但未详细阐释的相关概念。
2. **证明补充**：详细解释部分证明的思路，并补充书中省略的证明过程。
3. **案例分享**：增加相关实例，帮助读者加深对抽象概念的理解。

鉴于《导引》第一章的内容简明易懂，《钥匙书》从第二章开始详细展开。

对我个人而言，《机器学习理论导引》与*Understanding Machine Learning*和*Foundations of Machine Learning*一样，都是既“无用”又“有用”的书籍。“无用”在于目前的经典机器学习理论尚难全面解释深度学习，尤其是现代生成式大模型的惊人表现。然而，我坚信未来的理论突破将基于现有研究成果，开创新的篇章。因此，分析结论可能并非最重要，真正宝贵的是其中蕴含的思想和分析思路。数学作为一种强有力的工具，能够帮助我们更深入地理解和探索。我期望未来的深度学习能够拥有更多坚实的理论支撑，从而更好地指导实践。正如费曼所言：“What I cannot create, I do not understand.”——“凡我不能创造，我就不能理解。”希望大家能从这些理论中获得启发，创造出更有意义的成果。

另一方面，这本书也让我认识到自身的不足。不同于传统的机器学习算法教材，本书要求读者具备良好的数学功底，通过数学工具从更抽象的角度分析机器学习算法的性质，而非算法本身。学习之路或许漫长，但正如《牧羊少年的奇幻漂流》中所言：“每个人的寻梦过程都是以‘新手的运气’为开端，又总是以‘对远征者的考验’收尾。”希望大家能坚持经历考验，最终实现自己的梦想。

自《钥匙书》v1.0 版本发布以来，受到了众多学习者的关注。我们也收到了许多关于教材内容的疑问。为进一步深入理解相关知识，并记录团队对机器学习理论相关书籍的学习过程，我们将持续对《钥匙书》进行不定期更新，期待大家的关注。

-- 王茂霖

## 关于《机器学习理论导引》的价值与应用

随着机器学习的蓬勃发展，**SOTA**（State-of-the-art，最先进技术）几乎成了评判算法优劣的唯一标准。然而，这种对表面表现的单一追求，往往忽视了支撑其背后的基础理论。正如硅谷投资人吴军在《数学之美》中所言，顶尖科学家通过理论设定学科的边界，赋予未来研究者方向和框架。1936年，图灵在其著名论文中奠定了可计算性理论的基础，定义了哪些问题可以通过算法解决。同样，机器学习研究者只有具备深厚的理论根基，才能在实践中面对瓶颈时不迷失方向，而是继续探索，甚至开辟新的研究领域。

理论研究的重要性在**没有免费午餐定理**（No Free Lunch Theorem）中得到了充分体现。该定理明确指出，不存在一种能够适用于所有问题的通用算法。虽然某些算法在特定领域或时间点可能表现出色，如神经网络的崛起，但其优势通常仅限于特定任务和环境。盲目追求某一种算法的短期成功，可能导致在长期发展中陷入困境。通过理论学习，研究者能够识别这些局限，避免在实践中一味追逐SOTA，而忽视更为长远的技术路线。

1987年，Robert Hecht-Nielsen证明了任何连续的多元函数都可以通过特定的三层神经网络实现。这一成果在神经网络领域引发了广泛讨论。随后，Vugar Ismailov进一步扩展了这一理论，证明即使是不连续的函数，也可以通过类似的神经网络实现。这些研究不仅揭示了神经网络在函数逼近中的强大能力，还为模型的泛化性能提供了新的理论支持。这再次说明，机器学习理论不仅帮助我们理解现有模型的潜力，还能为未来的算法设计和改进奠定坚实的基础。

然而，理论与实践之间的差距始终存在。许多理论假设在现实应用中可能难以完全成立，特别是在处理大数据和复杂模型时，理论的指导似乎显得力不从心。然而，这并不意味着理论无用，恰恰相反，这正是推动学科发展的驱动力。机器学习的发展史表明，当实践遭遇瓶颈时，往往是理论的创新带来了新的突破。例如，在早期，受限于数据和算力，理论研究主导了机器学习的发展；而在互联网时代，随着数据量的指数级增长和计算资源的提升，实践逐渐超越理论。如今，面对数据、能源和算力的挑战，理论再次成为优化模型效率和提升算法性能的关键。

每当实践遇到瓶颈，理论创新总能带来突破。Transformer模型便是一个典型例子。近年来，Transformer凭借其强大的任务处理能力成为机器学习领域的明星模型。近期在ICLR 2024大会上，斯隆奖得主马腾宇及其团队通过数学方法证明，Transformer模型可以模拟任意多项式规模的数字电路。这一成果为其在复杂任务中的潜力提供了理论支持。与此同时，Chulhee Yun等人通过严谨的数学分析，证明了Transformer模型是紧支撑的连续置换等变序列到序列函数的通用逼近器。这些研究进一步揭示了Transformer模型在更广泛任务中的成功背后深层次的理论基础。这些成果不仅帮助我们更深入理解复杂模型的本质，还为未来的模型设计和优化提供了新方向。

理论学习不仅帮助我们理解模型的潜在能力，还为算法的泛化能力提供了深入洞见。通过研究机器学习理论，可以推导出算法在不同假设条件下的性能极限。例如，理论研究可以帮助我们评估算法的收敛速度，预测其在不同数据量和模型复杂度下的表现。这些理论工具不仅提高了研究的严谨性，还为实际应用提供了重要的指导。例如，通过理论分析，我们理解了为什么大规模语言模型的训练需要如此庞大的数据集，以及在某些任务上微调模型的优势。

结合神经网络和Transformer模型的理论进展，我们可以看到，理论贯穿了机器学习发展的每一个阶段。从最初的多层感知器到如今的大规模语言模型，理论研究不仅为算法性能提升提供了保障，也为未来技术发展奠定了稳固的基石。在追求SOTA的过程中，我们不能忽视理论的重要性，因为深刻的理论洞见是应对长期挑战、推动技术演进的关键。

最后，掌握机器学习理论不仅能够为初学者奠定坚实的基础，增强他们的信心，还能帮助他们在面对外界质疑时保持理性和清醒的判断。无论是在研究中提升算法，还是在实践中应对现实挑战，理论的力量都不可或缺。在本书的编撰中，我们特别对部分证明进行了必要的诠释和展开，主要集中于原书中流畅性不足的内容，或那些虽提供了参考文献但证明篇幅不超过5页的论述。对于超出5页的文献，我们建议读者直接参阅原文，以便更深入地探究；此类情况在本书中出现频率较低，约不超过五处。

通过深入学习机器学习理论，我们不仅能够更好地理解当前的挑战，还能认识到这些问题实际上是迈向**通用人工智能**（Artificial General Intelligence，AGI）过程中必须面对和解决的关键节点。

——赵志民

## 项目成员贡献与特别鸣谢

[詹好](https://github.com/zhanhao93)负责了项目的初期规划与统筹，并参与了第一版的编辑和审核；[赵志民](https://github.com/zhimin-z)主导了项目二期的更新与维护，并负责全书最终编辑和校验；[李一飞](https://github.com/leafy-lee)参与了第1-5章内容的编辑；[王茂霖](https://github.com/mlw67)参与了第2-6章内容的编辑。

另外，特别鸣谢了[谢文睿](https://github.com/Sm1les)和[杨昱文](https://github.com/youngfish42)，他们共同提供了本书的在线阅读功能；[张雨](https://github.com/Drizzle-Zhang)对第2章的早期内容进行了修订，各成员的协作确保了本书高质量的编写和顺利完成。
